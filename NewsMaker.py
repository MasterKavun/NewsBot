import mechanicalsoup
import sqlite3
import httpx
import asyncio
import json
import re
from config import API_KEY, AGENT_ID, API_URL

headers = {
   "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json" 
}

connection = sqlite3.connect("news.db")
cursor = connection.cursor()

# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–∞–±–ª–∏—Ü—å
cursor.execute("""
    CREATE TABLE IF NOT EXISTS news (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        title TEXT UNIQUE,
        url TEXT UNIQUE,
        image_url TEXT        
    )
""")

cursor.execute("""
  CREATE TABLE IF NOT EXISTS news_texts (
    news_id INTEGER PRIMARY KEY,
    full_text TEXT,
    processed_text TEXT,
    FOREIGN KEY(news_id) REFERENCES news(id)
  )
""")

cursor.execute("""
    CREATE TABLE IF NOT EXISTS posted_news (
        news_id INTEGER PRIMARY KEY,
        FOREIGN KEY(news_id) REFERENCES news(id)    
    )
""")

cursor.execute("""
    CREATE TABLE IF NOT EXISTS failed_news (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        news_id INTEGER UNIQUE,
        error_message TEXT,
        failed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
""")

def get_all_image_urls(article_page):
    image_urls = []

    # –û—Å–Ω–æ–≤–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
    main_img_tag = article_page.select_one("figure.article-body__figure img")
    if main_img_tag and main_img_tag.get("src"):
        src = main_img_tag["src"]
        high_res = re.sub(r"w=\\d+", "w=1280", src)
        image_urls.append(high_res)

    # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è ‚Äî <a href=...>
    extra_figures = article_page.select("figure.article__figure a")
    for a_tag in extra_figures:
        href = a_tag.get("href")
        if href and href.startswith("http"):
            image_urls.append(href) 

    return image_urls

# –ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç—É
url = "https://news.liga.net/ua"
browser = mechanicalsoup.StatefulBrowser()
browser.open(url)
page = browser.get_current_page()
articles = page.find_all("a", {"class":"news-card__title"})

for article in articles:
    title = article.text.strip()
    url = article["href"]

    browser.open(url)
    article_page = browser.get_current_page()

    # –ó–±—ñ—Ä —É—Å—ñ—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å —á–µ—Ä–µ–∑ —Ñ—É–Ω–∫—Ü—ñ—é
    image_urls = get_all_image_urls(article_page)
    image_urls_json = json.dumps(image_urls)

    if url.startswith("https://news.liga.net/"):
        try:
            cursor.execute("INSERT INTO news (title, url, image_url) VALUES (?,?,?)",
                        (title, url, image_urls_json))
            connection.commit()
        except sqlite3.IntegrityError:
            print(f"–ù–æ–≤–∏–Ω–∞ –≤–∂–µ —î –≤ –±–∞–∑—ñ: {title}")

connection.commit()
print("‚úÖ –ë–∞–∑—É –æ–Ω–æ–≤–ª–µ–Ω–æ")

cursor.execute("""
    SELECT id, url FROM news
    WHERE id NOT IN (SELECT news_id FROM news_texts)
""")

rows = cursor.fetchall()

for news_id, url in rows:
    try:
        browser.open(url)
        page = browser.get_current_page()
        article_text = page.find_all("p")
        full_text = '\n\n'.join([p.get_text().strip() for p in article_text if p.get_text().strip()])

        cursor.execute("""
            INSERT INTO news_texts (news_id, full_text, processed_text)
            VALUES (?,?, NULL)
        """, (news_id, full_text))
        connection.commit()
        print(f"–ó–±–µ—Ä–µ–∂–µ–Ω–æ —Ç–µ–∫—Å—Ç –Ω–æ–≤–∏–Ω–∏ ID {news_id}")
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ –Ω–æ–≤–∏–Ω–∏ ID {news_id}: {e}")

print("üì• –ü–∞—Ä—Å–∏–Ω–≥ —Ç–µ–∫—Å—Ç—ñ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

# –û–±—Ä–æ–±–∫–∞ —á–µ—Ä–µ–∑ Mistral
cursor.execute("""
    SELECT news_id, full_text FROM news_texts
    WHERE processed_text IS NULL
""")

news_to_process = cursor.fetchall()
connection.commit()

async def process_news(session, news_id, full_text):
    try:
        data = {
            "agent_id": AGENT_ID,
            "max_tokens": 850,
            "messages": [
                {"role": "user", "content": full_text}
            ]
        }

        response = await session.post(
            API_URL,
            headers=headers,
            json=data,
            timeout=60
        )
        response.raise_for_status()

        try:
            result = response.json()
            processed_text = result.get("choices", [{}])[0].get("message", {}).get("content", "")
        except Exception:
            print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ —Ä–æ–∑–±–æ—Ä—É JSON –¥–ª—è –Ω–æ–≤–∏–Ω–∏ ID {news_id}")
            return

        if processed_text:
            cursor.execute("""
                UPDATE news_texts
                SET processed_text = ?
                WHERE news_id = ?
            """, (processed_text, news_id))
            connection.commit()
            print(f"‚úÖ –ù–æ–≤–∏–Ω–∞ ID {news_id} –æ–±—Ä–æ–±–ª–µ–Ω–∞")
        else:
            print(f"‚ö†Ô∏è –í—ñ–¥—Å—É—Ç–Ω—ñ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –Ω–æ–≤–∏–Ω–∏ ID {news_id}")
    except Exception as e:
        print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ –Ω–æ–≤–∏–Ω–∏ ID {news_id}: {e}")

async def main():
    async with httpx.AsyncClient() as session:
        for news_id, full_text in news_to_process:
            await process_news(session, news_id, full_text)
            await asyncio.sleep(2)

asyncio.run(main())
connection.close()
print("üéâ –£—Å—ñ –Ω–æ–≤–∏–Ω–∏ –æ–±—Ä–æ–±–ª–µ–Ω–æ —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–æ")
